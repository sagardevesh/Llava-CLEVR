#!/usr/bin/env python
import argparse
import json
import os
import re

import torch
from PIL import Image
from transformers import AutoProcessor, LlavaForConditionalGeneration


def normalize_answer(ans):
    """Lowercase and remove punctuation for a simple match."""
    ans = ans.lower().strip()
    # Remove punctuation using regex
    ans = re.sub(r'[^\w\s]', '', ans)
    return ans


def extract_core_answer(generated_text, question_text):
    """
    Extract the core answer from a detailed LLaVA response based on question type.
    This improved function considers the question type when extracting answers.
    
    Args:
        generated_text: The full text generated by LLaVA
        question_text: The original question to help determine answer type
    
    Returns:
        The extracted core answer as a string
    """
    # Strip any prefixes like "ER: " or "ASSISTANT: " and the question repetition
    text = re.sub(r'^(USER:?\s*|ER:?\s*|ASSISTANT:?\s*)', '', generated_text, flags=re.IGNORECASE)
    
    # Remove any repetition of the question itself
    question_pattern = re.escape(question_text.strip())
    text = re.sub(question_pattern, '', text, flags=re.IGNORECASE).strip()
    if text.startswith(':'):
        text = text[1:].strip()
    
    # Determine question type to extract appropriate answer
    question_lower = question_text.lower()
    
    # For "what is X made of" questions - extract material
    if re.search(r'what.+made of', question_lower):
        material_match = re.search(r'\b(metal|rubber|metallic|matte|shiny)\b', text, re.IGNORECASE)
        if material_match:
            material = material_match.group(0).lower()
            if material == 'metallic' or material == 'shiny':
                return 'metal'
            if material == 'matte':
                return 'rubber'
            return material
    
    # For "what shape" or "what is the shape" questions
    if re.search(r'what.+(shape|form)', question_lower):
        shape_match = re.search(r'\b(cube|sphere|cylinder|block|cube-shaped|spherical|cylindrical)\b', text, re.IGNORECASE)
        if shape_match:
            shape = shape_match.group(0).lower()
            if shape in ['block', 'cube-shaped']:
                return 'cube'
            if shape == 'spherical':
                return 'sphere'
            if shape == 'cylindrical':
                return 'cylinder'
            return shape
    
    # For "what color" or "what is the color" questions
    if re.search(r'what.+color', question_lower):
        color_match = re.search(r'\b(red|blue|green|yellow|purple|brown|gray|cyan|black|white|gold)\b', text, re.IGNORECASE)
        if color_match:
            return color_match.group(0).lower()
    
    # For size questions
    if re.search(r'what.+size', question_lower):
        size_match = re.search(r'\b(large|big|small|tiny)\b', text, re.IGNORECASE)
        if size_match:
            size = size_match.group(0).lower()
            if size in ['big']:
                return 'large'
            if size in ['tiny']:
                return 'small'
            return size
    
    # For yes/no questions (must check this after more specific question types)
    if question_lower.startswith(('is ', 'are ', 'does ', 'do ', 'has ', 'have ', 'can ')):
        # First check for explicit yes/no
        if re.search(r'\b(yes|yeah|correct|right|true)\b', text, re.IGNORECASE):
            return "yes"
        if re.search(r'\b(no|not|isn\'t|aren\'t|doesn\'t|don\'t|cannot|can\'t|false)\b', text, re.IGNORECASE):
            return "no"
        
        # Look for affirmative structures
        if re.search(r'there is', text, re.IGNORECASE):
            return "yes"
        if re.search(r'there is no|there isn\'t|there are no|there aren\'t', text, re.IGNORECASE):
            return "no"
    
    # For number questions (how many)
    if re.search(r'how many', question_lower):
        number_match = re.search(r'\b(zero|one|two|three|four|five|six|seven|eight|nine|ten|0|1|2|3|4|5|6|7|8|9|10)\b', text, re.IGNORECASE)
        if number_match:
            num_word = number_match.group(0).lower()
            # Convert word to digit if needed
            number_mapping = {
                'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',
                'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10'
            }
            return number_mapping.get(num_word, num_word)
    
    # As a fallback for complex cases, try to find the final/concluding statement
    sentences = text.split('.')
    last_sentence = sentences[-1] if sentences[-1].strip() else sentences[-2] if len(sentences) > 1 else text
    
    # For material questions as fallback
    if 'made of' in question_lower:
        for material in ['metal', 'rubber']:
            if material in last_sentence.lower():
                return material
    
    # For shape questions as fallback
    if 'shape' in question_lower:
        for shape in ['cube', 'sphere', 'cylinder']:
            if shape in last_sentence.lower():
                return shape
    
    # If we get here and still haven't found an answer, look for key patterns in the full text
    if 'made of' in question_lower:
        for material in ['metal', 'rubber']:
            if material in text.lower():
                return material
                
    if 'shape' in question_lower:
        for shape in ['cube', 'sphere', 'cylinder']:
            if shape in text.lower():
                return shape
    
    # If we still can't extract a specific answer, return the shortest sentence
    # This is a last resort for complex answers
    shortest_sentence = min([s.strip() for s in sentences if s.strip()], key=len)
    return shortest_sentence


def run_inference(model, processor, image_path, question_text):
    try:
        image = Image.open(image_path).convert("RGB")
    except Exception as e:
        print(f"Error opening image {image_path}: {e}")
        return None

    prompt = f"USER: <image>\n{question_text}\nASSISTANT:"
    inputs = processor(images=image, text=prompt, return_tensors='pt').to(model.device, torch.float16)
    
    output = model.generate(**inputs, max_new_tokens=200, do_sample=False)
    generated_text = processor.decode(output[0][2:], skip_special_tokens=True).strip()
    return generated_text


def main(args):
    # Load test questions JSON file.
    with open(args.questions_file, 'r') as f:
        data = json.load(f)
    
    # Check the structure of the CLEVR questions file
    if "questions" in data:
        questions = data["questions"]
    else:
        questions = data  # Assume it's already a list

    if not questions:
        print("No questions found in the JSON file.")
        return

    # Setup device.
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load the LLava model.
    model = LlavaForConditionalGeneration.from_pretrained(
        "/fs01/model-weights/llava-1.5-13b-hf",
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    ).to(device)
    
    processor = AutoProcessor.from_pretrained("/fs01/model-weights/llava-1.5-13b-hf")
    
    # Explicitly set the patch_size - this is crucial for LLaVA 1.5
    processor.patch_size = 14
    
    # Ensure the vision config has the right image size
    if hasattr(model.config, "vision_config"):
        print(f"Model vision config: image_size={model.config.vision_config.image_size}, patch_size={model.config.vision_config.patch_size}")
        # Ensure processor patch size matches model config
        if hasattr(model.config.vision_config, "patch_size"):
            processor.patch_size = model.config.vision_config.patch_size
            print(f"Set processor patch_size to {processor.patch_size}")
    
    os.makedirs(args.output_dir, exist_ok=True)
    results = []
    correct_count = 0
    total_count = 0

    # Process a small batch first to test
    test_limit = args.test_limit if hasattr(args, 'test_limit') else None
    
    for idx, q in enumerate(questions):
        if test_limit and idx >= test_limit:
            break
            
        # CLEVR JSON structure handling
        if "image_filename" in q:
            image_file = q["image_filename"]
        elif "image_index" in q:
            # CLEVR format often uses indices instead of filenames
            image_index = q["image_index"]
            image_file = f"CLEVR_val_{image_index:06d}.png"
        else:
            print(f"Cannot determine image filename from question: {q}")
            continue
            
        question_text = q.get("question")
        ground_truth = q.get("answer", "")  # Default to empty if not found

        if not question_text:
            print(f"Skipping a question due to missing question field: {q}")
            continue

        image_path = os.path.join(args.input_dir, image_file)
        if not os.path.exists(image_path):
            print(f"Image not found: {image_path}")
            continue
            
        print(f"Processing image: {image_path}")
        try:
            generated_answer = run_inference(model, processor, image_path, question_text)
        except Exception as e:
            print(f"Error during inference: {e}")
            continue
            
        if generated_answer is None:
            continue

        total_count += 1
        
        # Extract the core answer from the generated text
        extracted_answer = extract_core_answer(generated_answer, question_text)
        
        # Only evaluate if ground truth is available
        is_correct = False
        if ground_truth:
            # Normalize answers for a simple exact match evaluation
            norm_extracted = normalize_answer(extracted_answer)
            norm_gt = normalize_answer(ground_truth)
            is_correct = norm_extracted == norm_gt
            if is_correct:
                correct_count += 1

        results.append({
            "image_file": image_file,
            "question": question_text,
            "ground_truth": ground_truth,
            "generated_answer": generated_answer,
            "extracted_answer": extracted_answer,
            "correct": is_correct
        })
        print(f"Question: {question_text}")
        print(f"Ground Truth: {ground_truth}")
        print(f"Full Generated: {generated_answer}")
        print(f"Extracted Answer: {extracted_answer}")
        if ground_truth:
            print(f"Correct: {is_correct}")
        print(f"{'-'*40}")

    # Compute and print overall accuracy.
    accuracy = correct_count / total_count if total_count > 0 else 0.0
    print(f"Total questions processed: {total_count}")
    print(f"Correct answers: {correct_count}")
    print(f"Accuracy: {accuracy:.2f}")

    # Save detailed results to a temporary file first to avoid disk quota issues
    temp_output_file = os.path.join(args.output_dir, "inference_results_temp.json")
    with open(temp_output_file, "w") as f:
        json.dump({
            "results": results,
            "summary": {
                "total": total_count,
                "correct": correct_count,
                "accuracy": accuracy
            }
        }, f)
    
    # Now rename to the final output file
    final_output_file = os.path.join(args.output_dir, "inference_results.json")
    os.rename(temp_output_file, final_output_file)
    print(f"Results saved to {final_output_file}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run LLava inference on CLEVR test set and evaluate accuracy.")
    parser.add_argument("--input_dir", type=str, required=True, help="Directory containing CLEVR test images.")
    parser.add_argument("--questions_file", type=str, required=True, help="Path to CLEVR_test_questions.json file.")
    parser.add_argument("--output_dir", type=str, required=True, help="Directory to save inference results and evaluation.")
    parser.add_argument("--test_limit", type=int, help="Limit the number of test examples (for debugging)")
    args = parser.parse_args()
    
    main(args)
